{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###DEVICE: cuda\n",
      "\n",
      "###DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "import glob\n",
    "import torch\n",
    "sys.path.append('../')\n",
    "import os\n",
    "from transformers import *\n",
    "from frameBERT.src import utils\n",
    "from frameBERT.src import dataio\n",
    "from frameBERT.src import eval_fn\n",
    "from frameBERT import frame_parser\n",
    "from frameBERT.src.modeling import BertForJointShallowSemanticParsing\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if device != \"cpu\":\n",
    "    torch.cuda.set_device(0)\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(0)   \n",
    "random.seed(0)\n",
    "\n",
    "from torch import autograd\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "srl = 'framenet'\n",
    "language = 'ko'\n",
    "fnversion = '1.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행시간 측정 함수\n",
    "import time\n",
    "\n",
    "_start_time = time.time()\n",
    "\n",
    "def tic():\n",
    "    global _start_time \n",
    "    _start_time = time.time()\n",
    "\n",
    "def tac():\n",
    "    t_sec = round(time.time() - _start_time)\n",
    "    (t_min, t_sec) = divmod(t_sec,60)\n",
    "    (t_hour,t_min) = divmod(t_min,60)\n",
    "    \n",
    "    result = '{}hour:{}min:{}sec'.format(t_hour,t_min,t_sec)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dir_path = os.path.dirname(os.path.abspath( __file__ ))\n",
    "except:\n",
    "    dir_path = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(PRETRAINED_MODEL=\"bert-base-multilingual-cased\",\n",
    "          model_dir=False, epochs=20, fnversion=False, early_stopping=True, batch_size=6):\n",
    "    \n",
    "    tic()\n",
    "    \n",
    "    if model_dir[-1] != '/':\n",
    "        model_dir = model_dir+'/'\n",
    "        \n",
    "    if early_stopping == True:\n",
    "        model_saved_path = model_dir+'best/'\n",
    "        model_dummy_path = model_dir+'dummy/'\n",
    "        if not os.path.exists(model_dummy_path):\n",
    "            os.makedirs(model_dummy_path)\n",
    "    else:\n",
    "        model_saved_path = model_dir        \n",
    "            \n",
    "    if not os.path.exists(model_saved_path):\n",
    "        os.makedirs(model_saved_path)\n",
    "    print('\\nyour model would be saved at', model_saved_path)\n",
    "\n",
    "    # load a pre-trained model first\n",
    "    print('\\nloading a pre-trained model...')\n",
    "    model = BertForJointShallowSemanticParsing.from_pretrained(PRETRAINED_MODEL, \n",
    "                                                               num_senses = len(bert_io.sense2idx), \n",
    "                                                               num_args = len(bert_io.bio_arg2idx),\n",
    "                                                               lufrmap=bert_io.lufrmap, \n",
    "                                                               frargmap = bert_io.bio_frargmap)\n",
    "    model.to(device)\n",
    "    print('... is done.', tac())\n",
    "    \n",
    "    print('\\nconverting data to BERT input...')\n",
    "    trn_data = bert_io.convert_to_bert_input_JointShallowSemanticParsing(trn)\n",
    "    sampler = RandomSampler(trn)\n",
    "    trn_dataloader = DataLoader(trn_data, sampler=sampler, batch_size=batch_size)\n",
    "    print('... is done', tac())\n",
    "    \n",
    "    # load optimizer\n",
    "    FULL_FINETUNING = True\n",
    "    if FULL_FINETUNING:\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'gamma', 'beta']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "    else:\n",
    "        param_optimizer = list(model.classifier.named_parameters()) \n",
    "        optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "    optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n",
    "    \n",
    "    max_grad_norm = 1.0\n",
    "    num_of_epoch = 0\n",
    "    \n",
    "    best_score = 0\n",
    "    renew_stack = 0\n",
    "    \n",
    "    for _ in trange(epochs, desc=\"Epoch\"):\n",
    "        \n",
    "        # TRAIN loop\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        for step, batch in enumerate(trn_dataloader):\n",
    "            model.train()\n",
    "            # add batch to gpu\n",
    "            torch.cuda.set_device(device)\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_orig_tok_to_maps, b_input_lus, b_input_senses, b_input_args, b_token_type_ids, b_input_masks = batch            \n",
    "            loss = model(b_input_ids, lus=b_input_lus, senses=b_input_senses, args=b_input_args,\n",
    "                     token_type_ids=b_token_type_ids, attention_mask=b_input_masks)\n",
    "            \n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # track train loss\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            \n",
    "            # gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "            \n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "        if early_stopping == True:\n",
    "            model.save_pretrained(model_dummy_path)\n",
    "            \n",
    "            # evaluate the model using dev dataset\n",
    "            print('\\n### eval by dev')\n",
    "            test_model = frame_parser.FrameParser(srl=srl, gold_pred=True, \n",
    "                                                  info=False, model_path=model_dummy_path, language=language)\n",
    "            parsed_result = []\n",
    "            \n",
    "            for instance in dev:\n",
    "                result = test_model.parser(instance)[0]\n",
    "                parsed_result.append(result)\n",
    "                \n",
    "            del test_model\n",
    "                \n",
    "            frameid, arg_precision, arg_recall, arg_f1, full_precision, full_recall, full_f1 = eval_fn.evaluate(dev, parsed_result)\n",
    "            d = {}\n",
    "            d['frameid'] = frameid\n",
    "            d['arg_precision'] = arg_precision\n",
    "            d['arg_recall'] = arg_recall\n",
    "            d['arg_f1'] = arg_f1\n",
    "            d['full_precision'] = full_precision\n",
    "            d['full_recall'] = full_recall\n",
    "            d['full_f1'] = full_f1\n",
    "            pprint(d)\n",
    "            print('Best score:', best_score)\n",
    "            \n",
    "            if full_f1 > best_score:\n",
    "                model.save_pretrained(model_saved_path)\n",
    "                best_score = full_f1\n",
    "                \n",
    "                renew_stack = 0\n",
    "            else:\n",
    "                renew_stack +=1\n",
    "        \n",
    "            # 성능이 3epoch 이후에도 개선되지 않으면 중단\n",
    "            if renew_stack >= 3:\n",
    "                break\n",
    "            \n",
    "        elif early_stopping == False:\n",
    "            # save your model for each epochs\n",
    "            model_saved_path = model_dir+str(num_of_epoch)+'/'\n",
    "            if not os.path.exists(model_saved_path):\n",
    "                os.makedirs(model_saved_path)\n",
    "            model.save_pretrained(model_saved_path)\n",
    "\n",
    "            num_of_epoch += 1\n",
    "        \n",
    "    print('...training is done. (', tac(), ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used dictionary:\n",
      "\t /disk/frameBERT/frameBERT/src/../koreanframenet/resource/info/kfn1.2_lu2idx.json\n",
      "\t /disk/frameBERT/frameBERT/src/../koreanframenet/resource/info/kfn1.2_lufrmap.json\n",
      "\t /disk/frameBERT/frameBERT/src/../koreanframenet/resource/info/mul_bio_frargmap.json\n",
      "\n",
      "### loading Korean FrameNet 1.2 data...\n",
      "\t# of instances in training data: 17838\n",
      "\t# of instances in dev data: 2548\n",
      "\t# of instances in test data: 5097\n",
      "[['태풍', 'Hugo가', '남긴', '피해들과', '회사', '내', '몇몇', '주요', '부서들의', '저조한', '실적들을', '반영하여,', 'Aetna', 'Life', 'and', 'Casualty', 'Co.의', '3분기', '<tgt>', '순이익이', '</tgt>', '182.6', '백만', '달러', '또는', '주당', '1.63', '달러로', '22', '%', '하락하였다.'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '이익.n', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'Earnings_and_losses', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Earner', 'I-Earner', 'I-Earner', 'I-Earner', 'I-Earner', 'B-Time', 'X', 'O', 'X', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "bert_io = utils.for_BERT(mode='train', language=language, masking=True, fnversion=fnversion)\n",
    "trn, dev, tst = dataio.load_data(language=language, fnversion=fnversion)\n",
    "print(trn[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "your model would be saved at /disk/frameBERT/models/test/best/\n",
      "\n",
      "loading a pre-trained model...\n",
      "... is done. 0hour:0min:5sec\n",
      "\n",
      "converting data to BERT input...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... is done 0hour:0min:31sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../frameBERT/src/utils.py:279: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred_logits = sm(masked_logit).view(1,-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### eval by dev\n",
      "evaluation is complete: 0hour:0min:0sec\n",
      "{'arg_f1': 0,\n",
      " 'arg_precision': 0.0,\n",
      " 'arg_recall': 0.0,\n",
      " 'frameid': 1.0,\n",
      " 'full_f1': 0.13793103448275865,\n",
      " 'full_precision': 0.08,\n",
      " 'full_recall': 0.5}\n",
      "Best score: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   5%|▌         | 1/20 [00:12<03:48, 12.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### eval by dev\n",
      "evaluation is complete: 0hour:0min:0sec\n",
      "{'arg_f1': 0,\n",
      " 'arg_precision': 0.0,\n",
      " 'arg_recall': 0.0,\n",
      " 'frameid': 1.0,\n",
      " 'full_f1': 0.28571428571428575,\n",
      " 'full_precision': 0.2,\n",
      " 'full_recall': 0.5}\n",
      "Best score: 0.13793103448275865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|█         | 2/20 [00:24<03:37, 12.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### eval by dev\n",
      "evaluation is complete: 0hour:0min:0sec\n",
      "{'arg_f1': 0,\n",
      " 'arg_precision': 0.0,\n",
      " 'arg_recall': 0.0,\n",
      " 'frameid': 1.0,\n",
      " 'full_f1': 0.4,\n",
      " 'full_precision': 0.3333333333333333,\n",
      " 'full_recall': 0.5}\n",
      "Best score: 0.28571428571428575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  15%|█▌        | 3/20 [00:36<03:24, 12.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### eval by dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|██        | 4/20 [00:44<02:52, 10.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation is complete: 0hour:0min:0sec\n",
      "{'arg_f1': 0,\n",
      " 'arg_precision': 0.0,\n",
      " 'arg_recall': 0.0,\n",
      " 'frameid': 1.0,\n",
      " 'full_f1': 0.4,\n",
      " 'full_precision': 0.3333333333333333,\n",
      " 'full_recall': 0.5}\n",
      "Best score: 0.4\n",
      "\n",
      "### eval by dev\n",
      "evaluation is complete: 0hour:0min:0sec\n",
      "{'arg_f1': 0,\n",
      " 'arg_precision': 0,\n",
      " 'arg_recall': 0.0,\n",
      " 'frameid': 1.0,\n",
      " 'full_f1': 0.6666666666666666,\n",
      " 'full_precision': 1.0,\n",
      " 'full_recall': 0.5}\n",
      "Best score: 0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  25%|██▌       | 5/20 [00:55<02:45, 11.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### eval by dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|███       | 6/20 [01:03<02:22, 10.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation is complete: 0hour:0min:0sec\n",
      "{'arg_f1': 0,\n",
      " 'arg_precision': 0,\n",
      " 'arg_recall': 0.0,\n",
      " 'frameid': 1.0,\n",
      " 'full_f1': 0.6666666666666666,\n",
      " 'full_precision': 1.0,\n",
      " 'full_recall': 0.5}\n",
      "Best score: 0.6666666666666666\n",
      "\n",
      "### eval by dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  35%|███▌      | 7/20 [01:11<02:03,  9.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation is complete: 0hour:0min:0sec\n",
      "{'arg_f1': 0,\n",
      " 'arg_precision': 0,\n",
      " 'arg_recall': 0.0,\n",
      " 'frameid': 1.0,\n",
      " 'full_f1': 0.6666666666666666,\n",
      " 'full_precision': 1.0,\n",
      " 'full_recall': 0.5}\n",
      "Best score: 0.6666666666666666\n",
      "\n",
      "### eval by dev\n",
      "evaluation is complete: 0hour:0min:0sec\n",
      "{'arg_f1': 0,\n",
      " 'arg_precision': 0,\n",
      " 'arg_recall': 0.0,\n",
      " 'frameid': 1.0,\n",
      " 'full_f1': 0.6666666666666666,\n",
      " 'full_precision': 1.0,\n",
      " 'full_recall': 0.5}\n",
      "Best score: 0.6666666666666666\n",
      "...training is done. ( 0hour:1min:50sec )\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "model_dir = '/disk/frameBERT/models/test'\n",
    "early_stopping = True\n",
    "batch_size = 6\n",
    "\n",
    "train(epochs=epochs, model_dir=model_dir, fnversion=fnversion, early_stopping=early_stopping, batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
